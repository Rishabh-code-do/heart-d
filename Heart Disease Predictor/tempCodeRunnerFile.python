
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


import hvplot.pandas
from scipy import stats


import seaborn as sns #statastical graph plotting


from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report



heart = pd.read_csv('heart.csv') #for reading csv files


heart.head() #first 5 entries

# age - age in years
# sex - (1 = male; 0 = female)
# cp - chest pain type
# 0: Typical angina: chest pain related decrease blood supply to the heart
# 1: Atypical angina: chest pain not related to heart
# 2: Non-anginal pain: typically esophageal spasms (non heart related)
# 3: Asymptomatic: chest pain not showing signs of disease
# trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern
# chol - serum cholestoral in mg/dl
# serum = LDL + HDL + .2 * triglycerides
# above 200 is cause for concern
# fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
# '>126' mg/dL signals diabetes
# restecg - resting electrocardiographic results
# 0: Nothing to note
# 1: ST-T Wave abnormality
# can range from mild symptoms to severe problems
# signals non-normal heart beat
# 2: Possible or definite left ventricular hypertrophy
# Enlarged heart's main pumping chamber
# thalach - maximum heart rate achieved
# exang - exercise induced angina (1 = yes; 0 = no)
# oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more
# slope - the slope of the peak exercise ST segment
# 0: Upsloping: better heart rate with excercise (uncommon)
# 1: Flatsloping: minimal change (typical healthy heart)
# 2: Downslopins: signs of unhealthy heart
# ca - number of major vessels (0-3) colored by flourosopy
# colored vessel means the doctor can see the blood passing through
# the more blood movement the better (no clots)
# thal - thalium stress result
# 1,3: normal
# 6: fixed defect: used to be defect but ok now
# 7: reversable defect: no proper blood movement when excercising
# target - have disease or not (1=yes, 0=no) (= the predicted attribute)


heart.nunique(axis=0)#unique values (0 for rowwise and 1 for clnwise)


pd.set_option("display.float", "{:.2f}".format)
heart.describe() #display some mathematical properties related to our data


x1=heart.target.unique() #stores the unique values in target col
y1=heart['target'].value_counts() # counts the distinct values in target



# Checking for messing values
heart.isna().sum() #check for NA values


categorical_val = []
continous_val = []
for column in heart.columns:
    if len(heart[column].unique()) <= 10:
        categorical_val.append(column)
    else:
        continous_val.append(column)


sns.barplot(x=x1 ,y=y1)


sns.countplot(x='age', hue = 'target',data = heart) #value be taken on x axis, value to be plotted, dataframe
fig=plt.gcf() #get current figure
fig.set_size_inches(16,10)


plt.figure(figsize=(15, 15))
#make a figure(width, height)
for i, column in enumerate(categorical_val, 1):
    plt.subplot(3, 3, i)
    heart[heart["target"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    heart[heart["target"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)


plt.figure(figsize=(15, 15))

for i, column in enumerate(continous_val, 1):
    plt.subplot(3, 2, i)
    heart[heart["target"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    heart[heart["target"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)

# # Age vs. Max Heart Rate for Heart Disease


# Create another figure
plt.figure(figsize=(9, 7))

# Scatter with positive examples
plt.scatter(heart.age[heart.target==1],
            heart.thalach[heart.target==1],
            c="salmon")

# Scatter with negative examples
plt.scatter(heart.age[heart.target==0],
            heart.thalach[heart.target==0],
            c="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);


corr = heart.corr() #returns the pairwise correlation
plt.subplots(figsize=(15,10))
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, 
            annot=True,
            cmap=sns.diverging_palette(220, 20,as_cmap=True))
#heatmap contains values representing various shades of the same colour for each value to be plotted.


heart.drop('target', axis=1).corrwith(heart.target).hvplot.barh(
    width=600, height=400, 
    title="Correlation between Heart Disease and Numeric Features", 
    ylabel='Correlation', xlabel='Numerical Features',
)


X = heart.iloc[:, :-1].values
y = heart.iloc[:, -1].values


# # MODEL BUILDING



#MODEL BUILDING
def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n")
        
    elif train==False:
        pred = clf.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))
        print("Test Result:\n================================================")        
        print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n")


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)


from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)


# # Logistic Regression


from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression(solver='liblinear')
lr_clf.fit(X_train, y_train)

print_score(lr_clf, X_train, y_train, X_test, y_test, train=True)
print_score(lr_clf, X_train, y_train, X_test, y_test, train=False)

# # K-nearest neighbors


from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)

print_score(knn_clf, X_train, y_train, X_test, y_test, train=True)
print_score(knn_clf, X_train, y_train, X_test, y_test, train=False)


# # Support Vector machine


from sklearn.svm import SVC


svm_clf = SVC(kernel='rbf', gamma=0.1, C=1.0)
svm_clf.fit(X_train, y_train)

print_score(svm_clf, X_train, y_train, X_test, y_test, train=True)
print_score(svm_clf, X_train, y_train, X_test, y_test, train=False)

# # Decision Tree Classifier


from sklearn.tree import DecisionTreeClassifier


tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)

print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)

# # Random Forest


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)


# # XGBoost Classifer


from xgboost import XGBClassifier

xgb_clf = XGBClassifier(use_label_encoder=False)
xgb_clf.fit(X_train, y_train)

print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)
print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)


# According to the above models the **Random Forest Classifier** has highest accuracy now we will use it for further predictions


# # Input For Predictions


# 0 - Represent Negative For Heart Diagnosis 
# 
# 1 - Represent Positive For Heart Diagnosis 


print(rf_clf.predict(sc.transform([[62,1,1,120,281,0,0,103,0,1.4,1,1,3]])))





print(rf_clf.predict(sc.transform([[20,1,2,110,230,1,1,140,1,2.2,2,0,2]])))





